{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b5e4e35e",
   "metadata": {},
   "source": [
    "## Remember to disable two-factor authentication if any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9adc85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5cdeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4be6e",
   "metadata": {},
   "source": [
    "This environment uses conda install --file requirements.txt to install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2783523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import logging\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0903b81",
   "metadata": {},
   "source": [
    "The following code demonstrate how easy it is to use a remote webdriver. Here we specify that we want to use a headless Chrome browser (browser without visual display) for our driver, which is hosted on port 4444 of our hub service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c34e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Remote(command_executor='http://hub:4444/wd/hub',\n",
    "                         desired_capabilities=DesiredCapabilities.CHROME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aee9cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n"
     ]
    }
   ],
   "source": [
    "# simle test to get Google title\n",
    "driver.get('https://www.google.com')\n",
    "print(driver.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b9696",
   "metadata": {},
   "source": [
    "## SETTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca24b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "LINKEDIN_USERNAME = os.getenv('LINKEDIN_USERNAME')\n",
    "LINKEDIN_PASSWORD = os.getenv('LINKEDIN_PASSWORD')\n",
    "\n",
    "class GEO_ID:\n",
    "    SG = \"102454443\"\n",
    "    MY = \"106808692\"\n",
    "    \n",
    "location = \"Singapore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2e099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait time for events in seconds\n",
    "page_wait = 30\n",
    "click_wait = 5\n",
    "async_wait = 5\n",
    "\n",
    "# when retrying, number of attempts\n",
    "attempts = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528c4f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xitonchong@gmail.com :  DD58sqUR5ifXcDq\n"
     ]
    }
   ],
   "source": [
    "print(LINKEDIN_USERNAME, \": \", LINKEDIN_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4cd5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_login = \"https://www.linkedin.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36d5f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_posted = \"r86400\" # filter for the past 24 hours\n",
    "search_posted = \"r2592000\" # filter for the past month\n",
    "search_remote = \"true\"\n",
    "search_location = \"Singapore\"\n",
    "search_keywords = ['Data%20Scientist', 'Data Analyst', 'Data Engineer']\n",
    "\n",
    "search_page = 0  # start on page 1\n",
    "search_count = 1 # initiate search count until looks on page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e9c41",
   "metadata": {},
   "source": [
    "### LOAD UTILS FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5639b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "logging = create_logfile()\n",
    "date = datetime.date.today().strftime('%d-%b-%y')\n",
    "file = f\"output/{date}_{search_location}.csv\"\n",
    "create_file(file, logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85defd41",
   "metadata": {},
   "source": [
    "## LOGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae65355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Logging in to LinkedIn as {LINKEDIN_USERNAME}...\")\n",
    "\n",
    "driver.get(url_login)\n",
    "driver.find_element_by_id(\"session_key\").send_keys(LINKEDIN_USERNAME)\n",
    "driver.find_element_by_id(\"session_password\").send_keys(LINKEDIN_PASSWORD)\n",
    "driver.find_element_by_xpath(\"//button[@class='sign-in-form__submit-button']\").click()\n",
    "\n",
    "try: \n",
    "    driver.find_element_by_xpath(\"//button[@class='primary-action-new']\").click()\n",
    "except:\n",
    "    print('failed')\n",
    "    pass\n",
    "logging.info(\"Log in complete. Scraping data...\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab12e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## page search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1160ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_search = f\"https://www.linkedin.com/jobs/search/?f_TPR={search_posted}&geoId={GEO_ID.SG}&keywords={search_keywords[0]}&location={search_location}&start={search_page}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "538943f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/jobs/search/?f_TPR=r2592000&geoId=102454443&keywords=Data%20Scientist&location=Singapore&start=0\n"
     ]
    }
   ],
   "source": [
    "print(url_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1075e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url_search)\n",
    "\n",
    "#time.sleep(10) # add sleep so don't get caught"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "345cdf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Data Scientist Jobs in Singapore | LinkedIn\n"
     ]
    }
   ],
   "source": [
    "print(driver.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fd16363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/jobs/search/?f_TPR=r2592000&geoId=102454443&keywords=Data%20Scientist&location=Singapore&start=0\n"
     ]
    }
   ],
   "source": [
    "# find the number of results\n",
    "print(driver.current_url)\n",
    "search_count = driver.find_element_by_xpath(\"//small[@class='display-flex t-12 t-black--light t-normal']\").text\n",
    "search_count = int(search_count.split(' ')[0].replace(',', ''))  # get number before space & remove comma (ex. \"1,245 results\")\n",
    "#logging.info(f\"Loading page {round(search_page/25) + 1} of {round(search_count/25)} for {search_keyword}'s {search_count} results...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb7f7304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2478\n"
     ]
    }
   ],
   "source": [
    "print(search_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de7af3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_count = driver.find_element_by_xpath(\"//small[@class='display-flex t-12 t-black--light t-normal']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dccdbc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_count = driver.find_element_by_xpath(\"//small[@class='display-flex t-12 t-black--light t-normal']\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6482c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_results = driver.find_element_by_xpath(\"//ul[@class='jobs-search-results__list list-style-none']\").find_elements_by_tag_name(\"li\")\n",
    "    result_ids = [result.get_attribute('id') for result in search_results if result.get_attribute('id') != '']\n",
    "except:\n",
    "    print('failed')\n",
    "    time.sleep(click_wait) # wait a few attempts, if not throw an exception and then skip to next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d96c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the job_id's for xpath for current page to click each element\n",
    "\n",
    "\n",
    "for attempt in range(attempts):\n",
    "    try:\n",
    "        search_results = driver.find_element_by_xpath(\"//ul[@class='jobs-search-results__list list-style-none']\").find_elements_by_tag_name(\"li\")\n",
    "        result_ids = [result.get_attribute('id') for result in search_results if result.get_attribute('id') != '']\n",
    "        break\n",
    "    except:\n",
    "        print('waiting')\n",
    "        time.sleep(click_wait) # wait a few attempts, if not throw an exception and then skip to next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3ce67b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ember219', 'ember231', 'ember245', 'ember259', 'ember273', 'ember285', 'ember297', 'ember311', 'ember312', 'ember313', 'ember314', 'ember315', 'ember316', 'ember317', 'ember318', 'ember319', 'ember320', 'ember321', 'ember322', 'ember323', 'ember324', 'ember325', 'ember326', 'ember327', 'ember328']\n"
     ]
    }
   ],
   "source": [
    "print(result_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c771799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/jobs/search/?f_TPR=r2592000&geoId=102454443&keywords=Data%20Scientist&location=Singapore&start=0\n"
     ]
    }
   ],
   "source": [
    "print(driver.current_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee1a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af2d7b1f",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccc9f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_search(wd, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging):\n",
    "    # wait time for events in seconds\n",
    "    page_wait = 30\n",
    "    click_wait = 5\n",
    "    async_wait = 5\n",
    "\n",
    "    # when retrying, number of attempts\n",
    "    attempts = 3\n",
    "\n",
    "    # navigate to search page\n",
    "    #url_search = f\"https://www.linkedin.com/jobs/search/?f_TPR={search_posted}&f_WRA={search_remote}&geoId=103644278&keywords={search_keyword}&location={search_location}&start={search_page}\"\n",
    "    url_search = f\"https://www.linkedin.com/jobs/search/?f_TPR={search_posted}&geoId={GEO_ID.SG}&keywords={search_keyword}&location={search_location}&start={search_page}\"\n",
    "    wd.get(url_search)\n",
    "    time.sleep(page_wait) # add sleep so don't get caught\n",
    "\n",
    "    # find the number of results \n",
    "    search_count = wd.find_element_by_xpath(\"//small[@class='display-flex t-12 t-black--light t-normal']\").text\n",
    "    search_count = int(search_count.split(' ')[0].replace(',', ''))  # get number before space & remove comma (ex. \"1,245 results\")\n",
    "    logging.info(f\"Loading page {round(search_page/25) + 1} of {round(search_count/25)} for {search_keyword}'s {search_count} results...\")\n",
    "\n",
    "    # get all the job_id's for xpath for current page to click each element\n",
    "    # running into errors with slow load (11-Aug)\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            search_results = wd.find_element_by_xpath(\"//ul[@class='jobs-search-results__list list-style-none']\").find_elements_by_tag_name(\"li\")\n",
    "            result_ids = [result.get_attribute('id') for result in search_results if result.get_attribute('id') != '']\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(click_wait) # wait a few attempts, if not throw an exception and then skip to next page\n",
    "\n",
    "    # cycle through each job_ids and steal the job data...muhahaha!\n",
    "    list_jobs = [] #initate a blank list to append each page to\n",
    "    for id in result_ids:\n",
    "        try:\n",
    "            job = wd.find_element_by_id(id) \n",
    "            job_id = job.get_attribute(\"data-occludable-entity-urn\").split(\":\")[-1]\n",
    "            # select a job and start extracting information\n",
    "            wd.find_element_by_xpath(f\"//div[@data-job-id={job_id}]\").click()\n",
    "        except:\n",
    "            continue\n",
    "            # exception likely to job deleteing need to go to next id\n",
    "\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                # from analysis 3 attempts at 5 second waits gets job titles 99.99% of time (11-Aug)\n",
    "                job_title = wd.find_element_by_xpath(\"//h2[@class='t-24 t-bold']\") # keep having issues with finding element\n",
    "                job_title = job_title.text\n",
    "                break\n",
    "            except:\n",
    "                job_title = ''\n",
    "                time.sleep(click_wait)\n",
    "        \n",
    "        # Having issues finding xpath of company (Added 11-Aug)\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                job_top_card1 = wd.find_element_by_xpath(\"//span[@class='jobs-unified-top-card__subtitle-primary-grouping mr2 t-black']\").find_elements_by_tag_name(\"span\")\n",
    "                company = job_top_card1[0].text\n",
    "                location = job_top_card1[1].text\n",
    "                if len(job_top_card1) > 2: # only displays remote if selected, otherwise only 2 elements in list\n",
    "                    remote = job_top_card1[2].text\n",
    "                else:\n",
    "                    remote = ''\n",
    "                break\n",
    "            except:\n",
    "                company = ''\n",
    "                location = ''\n",
    "                remote = ''\n",
    "                time.sleep(click_wait)\n",
    "        \n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                #multiple issues with job_top_card2 loading\n",
    "                job_top_card2 = wd.find_element_by_xpath(\"//span[@class='jobs-unified-top-card__subtitle-secondary-grouping t-black--light']\").find_elements_by_tag_name(\"span\")\n",
    "                update_time = job_top_card2[0].text\n",
    "                applicants = job_top_card2[1].text.split(' ')[0]\n",
    "                break\n",
    "            except: \n",
    "                update_time = '' # after #attempts leave as blank and move on\n",
    "                applicants = '' # after #attempts leave as blank and move on\n",
    "                time.sleep(click_wait)\n",
    "\n",
    "        # Due to (slow) ASYNCHRONOUS updates, need wait times to get job_info\n",
    "        job_time = '' # assigning as blanks as not important info and can skip if not obtained below\n",
    "        job_position = ''\n",
    "        job_pay = ''\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                # 1 - make sure HTML element is loaded\n",
    "                element = WebDriverWait(wd, 10).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='mt5 mb2']/div[1]\")))\n",
    "                # 2 - make sure text is loaded\n",
    "                try:\n",
    "                    job_info = element.text\n",
    "                    if job_info != '':\n",
    "                        # seperate job information on time requirements and position\n",
    "                        job_info = job_info.split(\" 路 \")\n",
    "                        if len(job_info) == 1: # only one item means its job _time\n",
    "                            job_pay = ''\n",
    "                            job_time = job_info[0]\n",
    "                            job_position = ''\n",
    "                        elif (len(job_info) >= 2) and (\"$\" in job_info[0]): # if has money symbol then seperate\n",
    "                            job_pay = job_info[0]\n",
    "                            job_time = job_info[1]\n",
    "                            if(len(job_info)>= 3): # check if job_info is required\n",
    "                                job_position = job_info[2]\n",
    "                            else:\n",
    "                                job_position = ''\n",
    "                        else: # else condition satisifies the last condition\n",
    "                            job_time = job_info[0]\n",
    "                            job_position = job_info[1]\n",
    "                            job_pay = ''\n",
    "                        break\n",
    "                    else:\n",
    "                        time.sleep(async_wait)\n",
    "                except:\n",
    "                    # error means page didn't load so try again\n",
    "                    time.sleep(async_wait)\n",
    "            except:\n",
    "                # error means page didn't load so try again\n",
    "                time.sleep(async_wait)\n",
    "\n",
    "        # get company details and seperate on size and industry\n",
    "        company_size = '' # assigning as blanks as not important info and can skip if not obtained below\n",
    "        company_industry = ''\n",
    "        job_details = ''      \n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                company_details = wd.find_element_by_xpath(\"//div[@class='mt5 mb2']/div[2]\").text\n",
    "                if \" 路 \" in company_details:\n",
    "                    company_size = company_details.split(\" 路 \")[0]\n",
    "                    company_industry = company_details.split(\" 路 \")[1]\n",
    "                else:\n",
    "                    company_size = company_details\n",
    "                    company_industry = ''\n",
    "                job_details = wd.find_element_by_id(\"job-details\").text.replace(\"\\n\", \" \")\n",
    "                break\n",
    "            except: \n",
    "                time.sleep(click_wait)\n",
    "\n",
    "        # append (a) line to file\n",
    "        date_time = datetime.datetime.now().strftime(\"%d%b%Y-%H:%M:%S\")\n",
    "        search_keyword = search_keyword.replace(\"%20\", \" \")\n",
    "        list_job = [date_time, search_keyword, search_count, job_id, job_title, company, location, remote, update_time, applicants, job_pay, job_time, job_position, company_size, company_industry, job_details]\n",
    "        list_jobs.append(list_job)\n",
    "\n",
    "    with open(file, \"a\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(list_jobs)\n",
    "        list_jobs = []\n",
    "    \n",
    "    logging.info(f\"Page {round(search_page/25) + 1} of {round(search_count/25)} loaded for {search_keyword}\")\n",
    "    search_page += 25\n",
    "\n",
    "    return search_page, search_count, url_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3da55",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be77ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL search terms focusing on what type of skills are required for Data Analyst & Data Scientist\n",
    "search_keywords = ['Data Analyst', 'Data Scientist', 'Data Engineer']\n",
    "\n",
    "\n",
    "# Counting Exceptions\n",
    "exception_first = 0\n",
    "exception_second = 0\n",
    "\n",
    "for search_keyword in search_keywords:\n",
    "    search_keyword = search_keyword.lower().replace(\" \", \"%20\")\n",
    "\n",
    "    # Loop through each page and write results to csv\n",
    "    search_page = 0 # start on page 1\n",
    "    search_count = 1 # initiate search count until looks on page\n",
    "    while (search_page < search_count) and (search_page != 1000):\n",
    "        # Search each page and return location after each completion\n",
    "        try:\n",
    "            search_page, search_count, url_search = page_search(driver, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging)\n",
    "        except Exception as e:\n",
    "            logging.error(f'(1) FIRST exception for {search_keyword} on {search_page} of {search_count}, retrying...')\n",
    "            logging.error(f'Current URL: {url_search}')\n",
    "            logging.error(e)\n",
    "            logging.exception('Traceback ->')\n",
    "            exception_first += 1\n",
    "            time.sleep(5) \n",
    "            try:\n",
    "                search_page, search_count, url_search = page_search(driver, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging)\n",
    "                logging.warning(f'Solved Exception for {search_keyword} on {search_page} of {search_count}')\n",
    "            except Exception as e:\n",
    "                logging.error(f'(2) SECOND exception remains for {search_keyword}. Skipping to next page...')\n",
    "                logging.error(f'Current URL: {url_search}')\n",
    "                logging.error(e)\n",
    "                logging.exception('Traceback ->')\n",
    "                search_page += 25 # skip to next page to avoid entry\n",
    "                exception_second += 1\n",
    "                logging.error(f'Skipping to next page for {search_keyword}, on {search_page} of {search_count}...')\n",
    "\n",
    "# close browser\n",
    "driver.quit()\n",
    "\n",
    "logging.info(f'LinkedIn data scraping complete with {exception_first} first and {exception_second} second exceptions')\n",
    "logging.info(f'Regard all further alarms...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61add161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cbfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
